{
  "section": "DQN_testbed",
  "seed": 43,
  "device": "cpu",
  "use_container": true,
  "env_config": {
    "world_name": "sequential_applr_testbed.world",
    "VLP16": "false",
    "gui": "false",
    "max_step": 600,
    "time_step": 1,
    "init_position": [-8, 0, 0],
    "goal_position": [54, 0, 0],
    "param_delta": [0.2, 0.3, 1, 2, 0.2, 0.2],
    "param_init": [0.1, 0.7, 6, 20, 0.75, 1],
    "param_list": ["max_vel_x", "max_vel_theta", "vx_samples", "vtheta_samples", "path_distance_bias", "goal_distance_bias"]
  },
  "wrapper_config": {
    "wrapper": "reward_shaping",
    "wrapper_args": {
      "start_range": [[-1.5, -0.5], [-1.5, 1.5]],
      "goal_range": [[0.5, 1.5],  [-1.5, 1.5]],
      "seed": 43,

      "reduction": 10,
      "polar_goal": "true",
      "centered_bin": "",
      "reward_shaping": "false",

      "goal_distance_reward": 2,
      "stuck_punishment": 0.5,
      "punishment_reward": -1000
    }
  },
  "training_config": {
    "num_envs": 8,
    "learning_rate": 0.001,
    "layer_num": 3,
    "hidden_layer_size": 128,
    "gamma": 0.95,
    "n_step": 1,
    "target_update_freq": 16,
    "prioritized_replay": false,
    "buffer_size": 20000,
    "epoch": 20,
    "step_per_epoch": 10000,
    "collect_per_step": 500,
    "update_per_step": 2000,
    "batch_size": 64,
    "exploration_ratio": 0.2
  }
}
